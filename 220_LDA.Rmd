##<a name="22_LDA" style="pointer-events: none; cursor: default;" ><font color="red"> 2.2. Linear Discriminant Analysis</font></a>

The linear discriminant analysis (LDA) consists of an alternative solution for building a discriminant function. Indeed, rewriting $P(Y=g|X)$ as $\frac{f_g(x)\pi_g}{\sum_{l=1}^{G}f_l(x)\pi_l}$ it is possible to estimate the probability of each class based on its density function (and its prior probability). Starting from this formulation, the linear discriminant analysis assumes that the $f_g(x)$ are $p+1$-dimensional gaussian distribution with common covariance matrix.

Once the $f_g(x)$ are estimated based on sample data, it is then possible to show that the discriminant function associated to each category turns out to be:
$$\delta_g(x)=x^{T}\Sigma^{-1}\mu_g -\frac{1}{2}\mu_g^T\Sigma^{-1}\mu_g+\log{\pi_g}$$
Finally, in order to classify observations into one of the classes we simply pick $g_*=argmax_g\delta_g(x)$.

**Risultati e plot distribuzione nella direzione discriminante**

```{r, fig.width=10, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/canonical_variable.Rdata"
load( file )

file = "results/MODELING/CLASSIFICATION/canonical_variable2.Rdata"
load( file )

subplot( canonical_variable, canonical_variable2) %>% layout(  title = 'Variabile Canonica')

```



Once the $f_g(x)$ are estimated based on sample data, it is then possible to show that the discriminant function associated to each category turns out to be:
$$\delta_g(x)=x^{T}\Sigma^{-1}\mu_g -\frac{1}{2}\mu_g^T\Sigma^{-1}\mu_g+\log{\pi_g}$$
Finally, in order to classify observations into one of the classes we simply pick $g_*=argmax_g\delta_g(x)$.

Solving this optimisation problem we get:
$$(W^{-1}B-\phi I_{G x G})a=0$$

This is a homogeneous system of linear equations, so it admits non-trivial solutions if and only if $(W^{-1}B-\phi I)=0$, i.e. if $\phi$ is an eigenvalue of $W^{-1}B$ and $a$ contains the correspondent eigenvector. Algebraically, this implies that we have at most $G-1$ solutions, precisely as many as the rank of $W^{-1}B$. As a consequence, there are no more than $G-1$ discriminant directions. Since $\phi$ is exactly the quantity we want to maximise, the best discriminant function is given by the eigenvector corresponding to the biggest eigenvalue of $W^{-1}B$. Namely, it is the surface that better separates one class from all the others.

In the case of two categories, we have a single non-zero eigenvalue and, hence, a single discriminant direction. To understand better the results of this approach, the following plot displays the observations with respect to the linear combination  $a^Tx$, also called canonic coordinate, with the linear decision boundary superimposed.

```{r, fig.width=10, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/lda_line_1_vs_0.Rdata"
load( file )
plot
```
Notice that the two classes do not appear very separated. Probably, the quite decent performances of the model are because the dataset is unbalanced and that the model correctly classifies the most represented categories. i.e. observations of class 1.

**verifica che le due formulazioni siano uguali**
