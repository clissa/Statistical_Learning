##<a name="21_LPM" style="pointer-events: none; cursor: default;" ><font color="red"> 2.1. Linear Probabity Model</font></a>

The regression model defines the linear discriminant function, $\delta(x)$, as the expected value of the target conditioning on the observed data, $E[Y|X]$. In case of a dichotomous outcome, this is equivalent to estimating the probability for an observation to belong to category 1, $P(Y=1|X)$, hence the name linear probability model.
However $E[Y|X]$ is by construction a real number, so it lends poorly in estimating a quantity that lies between 0 and 1. Nonetheless, it is possible to show that the sum of the probabilities to belong to every single class is 1.

To see why this is true, consider the general case of a multinomial classification problem (G categories) based on *p* regressors.
Then, the LPM approach consists of adopting G binary regression models simultaneously, one per each class. The issue then resolves to estimating the probability of belonging to each category and classifying the observation as the class which returns the highest response.
In matricial form:

$$\hat{Y}_{n x G} = X_{n x (p+1)} * ({X}^{T} X)_{(p+1) x (p+1)}^{-1} * {X}_{(p+1) x n}^{T} * Y_{n x G}$$

Post-multiplying by a G-dimensional column vector of ones we can then sum the elements of $\hat{Y}$:

$$\hat{Y}_{n x G} * 1_{G x 1} = X_{n x (p+1)} * ({X}^{T} X)_{(p+1) x (p+1)}^{-1} * {X}_{(p+1) x n}^{T} * Y_{n x G} * 1_{G x 1}$$

Now, pre-multiplying the identity matrix of order $n$ written as $(X {X}^{T})^{-1}(X {X}^{T})$, it is easy to verify that the previous equation resolves to:

$$(X {X}^{T})^{-1}(X {X}^{T}) * X_{n x (p+1)} * ({X}^{T} X)_{(p+1) x (p+1)}^{-1} * {X}_{(p+1) x n}^{T} * Y_{n x G} * 1_{G x 1}$$

$$(X {X}^{T})^{-1}X [({X}^{T} * X_{n x (p+1)}) * ({X}^{T} X)_{(p+1) x (p+1)}^{-1}] * {X}_{(p+1) x n}^{T} * Y_{n x G} * 1_{G x 1}$$

$$[(X {X}^{T})^{-1}X * {X}_{(p+1) x n}^{T}] * Y_{n x G} * 1_{G x 1}$$

$$\hat{Y}_{n x G} * 1_{G x 1} = Y_{n x G} * 1_{G x 1}$$

Hence the sum of the predicted scores is 1 for all the observations. In fact, $\hat{Y_1}+ ... + \hat{Y_G} = 1$ by construction.

For this reason, although it is not possible to interpret the predicted scores as probabilities, they share this property.

Passing to the application, we adopted such model considering all of the initial regressor.

**codice modello**

Of course, the predicted scores are not guaranteed to lie between 0 and 1, as anticipated:

```{r, fig.width=10, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/lpm_probs.Rdata"
load( file )
plot
```

Once a score has been obtained, it is the necessary to fix a cutoff so to classify each observation into one of the two categories. This choice is not that obvious, as the outcome of the model is not bounded in $[0,1]$. For this reason, the choice of the optimal threshold is don based on a grid search so to minimise the misclassification error.

**qui va inserita la parte di ottimizzazione treshold**

**terminare con risulati di accuratezza e ROC**
